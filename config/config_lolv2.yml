data:
  traindir: '../datasets/LOL-v2/Real_captured/Train/Low/'
  testdir: '../datasets/LOL-v2/Real_captured/Test/Low/' # or null

  train_csv_file: '../datasets/LOL-v1/LOL-v1_train_combined.csv'
  val_csv_file: None
  test_csv_file: '../datasets/LOL-v1/test.csv'

  train_mean: [0.0070, 0.0061, 0.0058]
  train_std:  [0.0064, 0.0056, 0.0057]
  val_mean:   [0.0053, 0.0059, 0.0055]
  val_std:    [0.0052, 0.0050, 0.0053]

  split_percentage: 1           # default: 0.9 (to only use training data use '1')
  task: 'low'                   # ['low', 'over', 'mixed']
  image_size: null              # or !tuple (900,600)
  input_patch_size: 256
  num_test_samples: null        # 3 Reducing the test size to fit into the memory

training:
  loss_funcs:
    # - 'L1'
    # - 'VGG'
    # - 'SSIM'
    # - 'LPIPS'
    # - 'YUV'
    - 'MSE'
    - 'HSV'
  
  epochs: 2 # 00
  batch_size: 20
  lr: 
  weight_decay: 
  seed: 3407
  testevery: 1
  scheduler: 'CosineAnnealingLR' ## ['LinearWarmupCosineAnnealingLR', 'StepLR', 'MultiStepLR', 'CosineAnnealingLR', 'WarmupCosine']
  # warmuprestartepochforcosine: [260] # Used for ConsineAnealling
  # ----------
  warmup_epochs: 10       # Used for Warmup-ConsineAnealling
  gamma: 0.5              # Used for StepLR scheduler
  step_size: 150          # Used for StepLR scheduler
  milestones: [400, 800]  # Used for MultiStepLR scheduler

model:
  checkpoint_dir: null # null / "path to the previous checkpoint"
  bilinear: True
  base_channels: 48
  deep_supervision: True

debugging:
  wandb: false
  save_model: false
  debug: false # true
  ckpt_dir: "checkpoint/lolv2/lolv2"
  tb: false